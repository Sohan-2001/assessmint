
'use server';
/**
 * @fileOverview AI flow to automatically evaluate exam submissions.
 *
 * - autoEvaluateSubmission - A function that processes a submission, awards marks, and generates feedback for each question.
 * - AutoEvaluateSubmissionInput - The input type for the autoEvaluateSubmission function.
 * - AutoEvaluateSubmissionOutput - The return type for the autoEvaluateSubmission function.
 */

import {ai} from '@/ai/genkit';
import {z} from 'genkit';

// Schema for a single question's evaluation input for the AI prompt
const AIQuestionEvaluationInputSchema = z.object({
  questionText: z.string().describe('The full text of the exam question.'),
  questionType: z.enum(['MULTIPLE_CHOICE', 'SHORT_ANSWER', 'ESSAY']).describe('The type of the question.'),
  questionPoints: z.number().positive().describe('The maximum points allocated for this question.'),
  userAnswerText: z.string().describe("The student's answer to the question. For MCQs, this will be the text of the option chosen by the student, or 'No answer provided' if none was selected."),
  optionsText: z.array(z.string()).optional().describe('For MCQs, an array of the text for each option (e.g., ["A) Paris", "B) London", ...]).'),
  correctAnswerText: z.string().optional().describe('The correct answer text. For MCQs, this is the text of the correct option. For Short Answers, this is the model answer.'),
});

// Schema for the AI prompt's output for a single question
const AIQuestionEvaluationOutputSchema = z.object({
  awardedMarks: z.number().min(0).describe('The marks awarded for this question by the AI. Must be between 0 and questionPoints.'),
  feedback: z.string().describe('Constructive feedback generated by the AI for the student for this specific question.'),
});


// Schema for the overall flow input
const QuestionDetailSchema = z.object({
  questionId: z.string(),
  questionText: z.string(),
  questionType: z.enum(['MULTIPLE_CHOICE', 'SHORT_ANSWER', 'ESSAY']),
  questionPoints: z.number().positive(),
  userAnswerText: z.string(), // This will be prepared by the calling server action
  optionsText: z.array(z.string()).optional(), // Prepared by server action
  correctAnswerText: z.string().optional(), // Prepared by server action
});
const AutoEvaluateSubmissionInputSchema = z.object({
  questions: z.array(QuestionDetailSchema).describe("An array of question details from the submission to be evaluated."),
});
export type AutoEvaluateSubmissionInput = z.infer<typeof AutoEvaluateSubmissionInputSchema>;


// Schema for the overall flow output
const EvaluatedAnswerSchema = z.object({
  questionId: z.string(),
  awardedMarks: z.number(),
  feedback: z.string(),
});
const AutoEvaluateSubmissionOutputSchema = z.object({
  evaluatedAnswers: z.array(EvaluatedAnswerSchema).describe("An array of AI-evaluated answers, each with marks and feedback."),
  totalScore: z.number().describe("The total score calculated from all awarded marks."),
});
export type AutoEvaluateSubmissionOutput = z.infer<typeof AutoEvaluateSubmissionOutputSchema>;


// Define the prompt for evaluating a single question
const evaluateAnswerPrompt = ai.definePrompt({
  name: 'evaluateAnswerPrompt',
  input: { schema: AIQuestionEvaluationInputSchema },
  output: { schema: AIQuestionEvaluationOutputSchema },
  prompt: `You are an AI exam evaluator. Your task is to evaluate a student's answer to a given question and provide marks and feedback.
Strictly adhere to the output JSON schema. The "awardedMarks" must not exceed "questionPoints".

Question Details:
- Text: {{{questionText}}}
- Type: {{{questionType}}}
- Max Points: {{{questionPoints}}}
{{#if correctAnswerText}}
- Correct Answer/Model Answer: {{{correctAnswerText}}}
{{/if}}
{{#if optionsText}}
- Options:
{{#each optionsText}}
  - {{{this}}}
{{/each}}
{{/if}}

Student's Answer:
{{{userAnswerText}}}

Evaluation Instructions:
1.  **Multiple Choice (MULTIPLE_CHOICE)**:
    *   If "{{{userAnswerText}}}" exactly matches "{{{correctAnswerText}}}", award full points ({{{questionPoints}}}). Feedback should be "Correct." or similar.
    *   Otherwise, award 0 points. Feedback should state the correct answer, e.g., "Incorrect. The correct answer was '{{{correctAnswerText}}}'."
    *   If user answer is 'No answer provided', award 0 points and state that no answer was given.
2.  **Short Answer (SHORT_ANSWER)**:
    *   Compare the student's answer with the model answer ("{{{correctAnswerText}}}") if provided.
    *   Assess correctness, completeness, and clarity.
    *   Award marks proportionally up to {{{questionPoints}}}. For example, if half correct, award half points.
    *   Provide specific feedback explaining why marks were awarded or deducted. If a model answer is available, reference it.
3.  **Essay (ESSAY)**:
    *   Evaluate based on relevance to the question, depth of understanding, organization, clarity, and use of examples (if applicable).
    *   Award marks proportionally up to {{{questionPoints}}}.
    *   Provide constructive feedback highlighting strengths and areas for improvement.

Awarded marks must be an integer and not exceed {{{questionPoints}}}.
Generate concise and helpful feedback.
Output ONLY the JSON for AIQuestionEvaluationOutputSchema.
`,
});

// Define the main flow
const autoEvaluateSubmissionFlow = ai.defineFlow(
  {
    name: 'autoEvaluateSubmissionFlow',
    inputSchema: AutoEvaluateSubmissionInputSchema,
    outputSchema: AutoEvaluateSubmissionOutputSchema,
  },
  async (input) => {
    const evaluatedAnswers: Array<z.infer<typeof EvaluatedAnswerSchema>> = [];
    let totalScore = 0;

    for (const q of input.questions) {
      const promptInput: z.infer<typeof AIQuestionEvaluationInputSchema> = {
        questionText: q.questionText,
        questionType: q.questionType,
        questionPoints: q.questionPoints,
        userAnswerText: q.userAnswerText,
        optionsText: q.optionsText,
        correctAnswerText: q.correctAnswerText,
      };

      try {
        const { output } = await evaluateAnswerPrompt(promptInput);
        if (output) {
          // Ensure awarded marks do not exceed question points (AI might sometimes hallucinate)
          const cappedMarks = Math.min(Math.max(0, Math.round(output.awardedMarks)), q.questionPoints);
          
          evaluatedAnswers.push({
            questionId: q.questionId,
            awardedMarks: cappedMarks,
            feedback: output.feedback,
          });
          totalScore += cappedMarks;
        } else {
          // Handle case where AI might not return output (should be rare with schema)
          evaluatedAnswers.push({
            questionId: q.questionId,
            awardedMarks: 0,
            feedback: "AI evaluation failed for this question.",
          });
        }
      } catch (error) {
        console.error(`Error evaluating question ${q.questionId} with AI:`, error);
        evaluatedAnswers.push({
          questionId: q.questionId,
          awardedMarks: 0,
          feedback: `AI evaluation encountered an error: ${error instanceof Error ? error.message : 'Unknown error'}. Please evaluate manually.`,
        });
      }
    }

    return {
      evaluatedAnswers,
      totalScore,
    };
  }
);

// Exported wrapper function
export async function autoEvaluateSubmission(input: AutoEvaluateSubmissionInput): Promise<AutoEvaluateSubmissionOutput> {
  return autoEvaluateSubmissionFlow(input);
}
